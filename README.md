# XandAI - Assistente Virtual Inteligente

## ğŸ¤– VisÃ£o Geral

XandAI Ã© um assistente virtual moderno e inteligente construÃ­do com React e Material-UI, featuring uma integraÃ§Ã£o completa com OLLAMA para modelos de IA locais. A aplicaÃ§Ã£o oferece uma interface de chat elegante com tema escuro e funcionalidades avanÃ§adas de gerenciamento de modelos.

## âœ¨ Principais Funcionalidades

### ğŸ¨ Interface Moderna
- **Tema Escuro**: Design elegante e moderno otimizado para reduzir fadiga visual
- **Responsivo**: Interface adaptÃ¡vel para desktop e mobile
- **Material-UI**: Componentes consistentes e acessÃ­veis
- **AnimaÃ§Ãµes Suaves**: TransiÃ§Ãµes e feedback visual aprimorados

### ğŸ§  IntegraÃ§Ã£o de IA
- **OLLAMA Integration**: Conecte-se com modelos de IA locais
- **Fallback AutomÃ¡tico**: Sistema inteligente que alterna entre OLLAMA e respostas mock
- **SeleÃ§Ã£o de Modelos**: Interface para escolher e gerenciar modelos disponÃ­veis
- **Status em Tempo Real**: Indicadores visuais do status da conexÃ£o e modelo

### ğŸ’¬ Chat AvanÃ§ado
- **Mensagens em Tempo Real**: Interface de chat fluida e responsiva
- **HistÃ³rico Persistente**: ManutenÃ§Ã£o do histÃ³rico de conversas
- **Indicadores de DigitaÃ§Ã£o**: Feedback visual durante o processamento
- **GestÃ£o de SessÃµes**: Controle completo sobre sessÃµes de chat

### âš™ï¸ ConfiguraÃ§Ã£o FlexÃ­vel
- **Painel de ConfiguraÃ§Ãµes**: Interface intuitiva para configurar OLLAMA
- **Teste de Conectividade**: VerificaÃ§Ã£o automÃ¡tica da disponibilidade do serviÃ§o
- **Gerenciamento de Modelos**: Download, seleÃ§Ã£o e remoÃ§Ã£o de modelos
- **ConfiguraÃ§Ã£o Persistente**: ConfiguraÃ§Ãµes salvas localmente

## ğŸ—ï¸ Arquitetura

O projeto segue os princÃ­pios de Clean Architecture com separaÃ§Ã£o clara de responsabilidades:

```
src/
â”œâ”€â”€ components/           # Componentes React reutilizÃ¡veis
â”‚   â”œâ”€â”€ chat/            # Componentes especÃ­ficos do chat
â”‚   â”œâ”€â”€ settings/        # ConfiguraÃ§Ãµes e painÃ©is
â”‚   â””â”€â”€ common/          # Componentes compartilhados
â”œâ”€â”€ application/         # Camada de aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ hooks/           # Custom hooks React
â”‚   â””â”€â”€ services/        # ServiÃ§os de negÃ³cio
â”œâ”€â”€ domain/              # Entidades e regras de negÃ³cio
â”‚   â”œâ”€â”€ entities/        # Modelos de dados
â”‚   â””â”€â”€ repositories/    # Interfaces de repositÃ³rio
â”œâ”€â”€ infrastructure/      # ImplementaÃ§Ãµes de infraestrutura
â”‚   â”œâ”€â”€ api/             # IntegraÃ§Ãµes com APIs externas
â”‚   â””â”€â”€ mock-api/        # ImplementaÃ§Ãµes mock
â””â”€â”€ styles/              # Temas e estilos globais
```

## ğŸš€ ComeÃ§ando

### PrÃ©-requisitos

- Node.js 16+ e npm/yarn
- OLLAMA instalado (opcional, para IA local)

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**
   ```bash
   git clone https://github.com/seu-usuario/xandai.git
   cd xandai
   ```

2. **Instale as dependÃªncias**
   ```bash
   npm install
   ```

3. **Inicie a aplicaÃ§Ã£o**
   ```bash
   npm start
   ```

4. **Acesse no navegador**
   ```
   http://localhost:3000
   ```

### ConfiguraÃ§Ã£o do OLLAMA (Opcional)

Para usar modelos de IA locais, configure o OLLAMA:

1. **Instale o OLLAMA**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Windows: Download do site oficial
   ```

2. **Inicie o serviÃ§o**
   ```bash
   ollama serve
   ```

3. **Baixe modelos**
   ```bash
   ollama pull llama2:latest
   ollama pull mistral:latest
   ```

4. **Configure no XandAI**
   - Clique no botÃ£o de configuraÃ§Ãµes no cabeÃ§alho
   - Habilite a integraÃ§Ã£o OLLAMA
   - Teste a conexÃ£o
   - Selecione um modelo

## ğŸ¯ Como Usar

### Interface Principal

1. **Seletor de Modelo**: No cabeÃ§alho, escolha entre Mock AI ou modelos OLLAMA
2. **Chat**: Digite mensagens na Ã¡rea de input
3. **ConfiguraÃ§Ãµes**: Acesse via botÃ£o de configuraÃ§Ãµes para gerenciar OLLAMA
4. **HistÃ³rico**: Use o botÃ£o "Limpar" para resetar a conversa

### ConfiguraÃ§Ãµes OLLAMA

1. **ConexÃ£o**: Configure a URL do OLLAMA (padrÃ£o: http://localhost:11434)
2. **Modelos**: Visualize, selecione e gerencie modelos disponÃ­veis
3. **Status**: Monitore a conectividade e status dos modelos
4. **Timeout**: Ajuste o tempo limite para requisiÃ§Ãµes

### Funcionalidades do Chat

- **Envio de Mensagens**: Digite e pressione Enter ou clique em "Enviar"
- **Fallback AutomÃ¡tico**: Se OLLAMA falhar, o sistema usa respostas mock automaticamente
- **HistÃ³rico**: Conversas sÃ£o mantidas durante a sessÃ£o
- **Indicadores**: Veja quando o assistente estÃ¡ "digitando"

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# URL padrÃ£o do OLLAMA
REACT_APP_OLLAMA_DEFAULT_URL=http://localhost:11434

# Timeout padrÃ£o (ms)
REACT_APP_OLLAMA_DEFAULT_TIMEOUT=30000

# Habilitar debug
REACT_APP_DEBUG=true
```

### CustomizaÃ§Ã£o de Temas

Edite `src/styles/theme/theme.js` para personalizar cores e estilos:

```javascript
export const customTheme = createTheme({
  palette: {
    primary: {
      main: '#sua-cor-primaria',
    },
    // ... outras configuraÃ§Ãµes
  }
});
```

## ğŸ“š DocumentaÃ§Ã£o

### DocumentaÃ§Ã£o Completa
- [IntegraÃ§Ã£o OLLAMA](docs/OLLAMA_INTEGRATION.md) - Guia completo da integraÃ§Ã£o OLLAMA
- [Arquitetura](docs/README.md) - Detalhes da arquitetura do sistema

### APIs e Schemas

#### Entidades Principais

**OllamaConfig**
```javascript
{
  baseUrl: string,     // URL do OLLAMA
  timeout: number,     // Timeout em ms
  selectedModel: string, // Modelo selecionado
  enabled: boolean     // Se estÃ¡ habilitado
}
```

**OllamaModel**
```javascript
{
  name: string,        // Nome do modelo
  size: number,        // Tamanho em bytes  
  family: string,      // FamÃ­lia (llama, mistral, etc.)
  tag: string,         // VersÃ£o/tag
  isAvailable: boolean // Disponibilidade
}
```

**Message**
```javascript
{
  id: string,          // ID Ãºnico
  content: string,     // ConteÃºdo da mensagem
  sender: 'user'|'assistant', // Remetente
  timestamp: Date,     // Timestamp
  isTyping: boolean    // Se Ã© mensagem de digitaÃ§Ã£o
}
```

## ğŸ› ï¸ Scripts DisponÃ­veis

### Desenvolvimento
```bash
npm start          # Inicia servidor de desenvolvimento
npm test           # Executa testes
npm run build      # Build para produÃ§Ã£o
npm run eject      # Ejeta configuraÃ§Ã£o (irreversÃ­vel)
```

### Linting e FormataÃ§Ã£o
```bash
npm run lint       # Verifica problemas de cÃ³digo
npm run format     # Formata cÃ³digo automaticamente
```

## ğŸ§ª Testes

Execute os testes automatizados:

```bash
# Testes unitÃ¡rios
npm test

# Testes com coverage
npm test -- --coverage

# Testes em modo watch
npm test -- --watch
```

## ğŸ“¦ Build e Deploy

### Build de ProduÃ§Ã£o
```bash
npm run build
```

### Deploy
O build gera arquivos estÃ¡ticos na pasta `build/` que podem ser servidos por qualquer servidor web:

```bash
# Servir localmente para teste
npx serve -s build

# Deploy para Netlify, Vercel, etc.
# FaÃ§a upload da pasta build/
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Diretrizes de ContribuiÃ§Ã£o

- Siga os padrÃµes de cÃ³digo estabelecidos
- Escreva testes para novas funcionalidades
- Documente mudanÃ§as importantes
- Use commits semÃ¢nticos

## ğŸ“‹ Roadmap

### PrÃ³ximas Funcionalidades
- [ ] **AutenticaÃ§Ã£o de UsuÃ¡rio**: Sistema de login e perfis
- [ ] **MÃºltiplas SessÃµes**: Gerenciamento de vÃ¡rias conversas
- [ ] **ExportaÃ§Ã£o de Conversas**: PDF, texto, etc.
- [ ] **Plugins Personalizados**: Sistema de extensÃµes
- [ ] **API REST**: Backend para persistÃªncia
- [ ] **SincronizaÃ§Ã£o em Nuvem**: Backup automÃ¡tico
- [ ] **Comandos de Voz**: IntegraÃ§Ã£o com speech-to-text
- [ ] **Modo Colaborativo**: Chat em grupo

### Melhorias TÃ©cnicas
- [ ] **PWA**: Progressive Web App
- [ ] **Offline Mode**: Funcionalidade offline
- [ ] **Performance**: OtimizaÃ§Ãµes de carregamento
- [ ] **Acessibilidade**: Melhorias de a11y
- [ ] **InternacionalizaÃ§Ã£o**: Suporte a mÃºltiplos idiomas

## ğŸ› Problemas Conhecidos

### OLLAMA
- Primeira execuÃ§Ã£o pode ser lenta (carregamento do modelo)
- Requer recursos significativos (CPU/GPU/RAM)
- Compatibilidade limitada a modelos suportados

### Interface
- Mobile precisa de otimizaÃ§Ãµes adicionais
- Alguns componentes podem nÃ£o funcionar em browsers antigos

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **OLLAMA Team** - Pela excelente ferramenta de IA local
- **Material-UI** - Pelo sistema de componentes
- **React Team** - Pelo framework incrÃ­vel
- **Comunidade Open Source** - Por inspiraÃ§Ãµes e contribuiÃ§Ãµes

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/seu-usuario/xandai/issues)
- **DiscussÃµes**: [GitHub Discussions](https://github.com/seu-usuario/xandai/discussions)
- **Email**: seu-email@exemplo.com

---

**XandAI** - Construindo o futuro das interfaces de IA, uma conversa por vez. ğŸš€